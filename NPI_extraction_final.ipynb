{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import conllu\n",
    "from conllu import parse, parse_tree, print_tree\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class for NPI candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Candidate:\n",
    "    def __init__(self, word, pos):\n",
    "        self.word = word\n",
    "        self.pos = pos\n",
    "        self.contexts = {'DN':0, 'DNN':0, 'RESTR': 0, 'COND':0, 'CONDIRR':0, 'QUEST':0, 'MODAL':0, 'IN':0, 'IMP':0, 'IRR': 0}\n",
    "        self.all_nonver = 0\n",
    "        self.all_occ = 0\n",
    "        self.all_other = 0\n",
    "        \n",
    "    def spotted(self, cont):\n",
    "        if cont is not 'VER':\n",
    "            self.contexts[cont] += 1\n",
    "            self.all_nonver += 1\n",
    "            \n",
    "    def is_npi(self):\n",
    "        n = self.all_nonver / self.all_occ\n",
    "        m = self.all_occ - self.all_nonver\n",
    "        if m > self.all_nonver:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    def set_other_c(self):\n",
    "        self.all_other = self.all_occ - self.all_nonver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning sentences into a dataframe with relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sent(sent):\n",
    "    tokens = []\n",
    "    parsed = parse(sent)\n",
    "    for x in parsed[0]:\n",
    "        if x['upostag'] == 'VERB' or x['upostag'] == 'PRON' or x['upostag'] == 'ADV':\n",
    "            gram = x['feats']\n",
    "        else:\n",
    "            gram = None\n",
    "        #print(x['form'])\n",
    "        token = [x['id'], x['head'], x['form'], x['lemma'], x['upostag'], gram, None]\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def parsed_to_df(tokens):\n",
    "    df = pd.DataFrame(tokens)\n",
    "    df.columns = ['t_id', 'head', 'form', 'lemma', 'pos', 'gram', 'is_in_scope']\n",
    "    #df.set_index(['token_id', 'head', 'form', 'lemma', 'pos'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions for work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate a sentence into clauses if there is direct speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_clauses(df):\n",
    "    dash = df.loc[df['lemma']=='-']\n",
    "    if dash.empty is False:\n",
    "        for index, row in dash.iterrows():\n",
    "            punct = df.loc[df['t_id']==(row['t_id']-1), 'pos']\n",
    "            if punct.max() == 'PUNCT':\n",
    "                #print(punct)\n",
    "                df1 = df.loc[df['t_id'] <= row['t_id']]\n",
    "                df2 = df.loc[df['t_id'] > row['t_id']]\n",
    "                return [df1, df2]\n",
    "            else:\n",
    "                return [df]\n",
    "    else:\n",
    "        return [df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect items if the scope goes down the UD Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope_down(df, token, depth=2):\n",
    "    scoped = []\n",
    "    children = df.loc[df['head']==token]\n",
    "    for index, row in children.iterrows():\n",
    "        if row['pos'] == 'VERB' or row['pos'] == 'ADV' or row['pos'] == 'PRON':\n",
    "            scoped.append([row['lemma'], row['pos'], row['t_id']])\n",
    "        if depth > 0:\n",
    "            depth -= 1\n",
    "            scoped1 = scope_down(df, row['t_id'])\n",
    "            if len(scoped1) > 0:\n",
    "                for x in scoped1:\n",
    "                    scoped.append(x)\n",
    "    return scoped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope_down_gram(df, token, depth=0):\n",
    "    scoped = []\n",
    "    children = df.loc[df['head']==token]\n",
    "    scoped = []\n",
    "    for index, row in children.iterrows():\n",
    "        if row['pos'] == 'VERB' or row['pos'] == 'ADV' or row['pos'] == 'PRON':\n",
    "            scoped.append([row['lemma'], row['pos'], row['gram']])\n",
    "        if depth > 0:\n",
    "            depth -= 1\n",
    "            scoped1 = scope_down(sent, row['t_id'])\n",
    "            if len(scoped1) > 0:\n",
    "                for x in scoped1:\n",
    "                    scoped.append(x)\n",
    "                scoped = list(itertools.chain(*scoped))\n",
    "    return scoped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect items if the scope goes up the UD Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope_up1(df, token): #for now we won't raise any further\n",
    "    parent_id = int(df.loc[df['t_id']==token, 'head'])\n",
    "    parent = df.loc[df['t_id']==parent_id]\n",
    "    return [parent['lemma'].max(), parent['pos'].max(), parent_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope_up1_gram(df, token): #for now we won't raise any further\n",
    "    parent_id = int(df.loc[df['t_id']==token, 'head'])\n",
    "    parent = df.loc[df['t_id']==parent_id]\n",
    "    scoped = [parent['lemma'].max(), parent['pos'].max(), parent['gram'].max()]\n",
    "    return scoped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope_up_far(df, token): #here we raise (by depth?)\n",
    "    scoped = []\n",
    "    depth = 4\n",
    "    parent_id = int(df.loc[df['t_id']==token, 'head'])\n",
    "    parent = df.loc[df['t_id']==parent_id]\n",
    "    for d in range(depth):\n",
    "        scoped1= scope_up_far(df, int(parent['head']))\n",
    "    if len(scoped1) > 0:\n",
    "        for x in scoped1:\n",
    "            scoped.append(x)\n",
    "    return scoped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count occurences of all lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_lemmas(parsed_sentences):\n",
    "    all_lemmas = []\n",
    "    for x in parsed_sentences:\n",
    "        all_lemmas.append(list(x['lemma']))\n",
    "    all_lemmas = list(itertools.chain(*all_lemmas))\n",
    "    lemmas = collections.Counter(all_lemmas)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special functions for particular contexts (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct & Indirect Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_neg(df, token):\n",
    "    scoped = []\n",
    "    if df.loc[df['t_id']==token, 'pos'].max() == 'VERB' and df.loc[df['t_id']==token, 'lemma'].max() != 'быть':\n",
    "        scoped = scope_down(df, token)\n",
    "        for s in scoped:\n",
    "            s.append('DN')\n",
    "    else:\n",
    "        negated1 = scope_up1(df, token)\n",
    "        negated1.append('DN')\n",
    "        if negated1 is not None:\n",
    "            scoped.append(negated1)\n",
    "        nn = df.loc[df['lemma']==negated1[0]]\n",
    "        for index, row in nn.iterrows():\n",
    "            if row['head'] == token:\n",
    "                negated2 = scope_down(df, row['t_id'], 4)\n",
    "                for x in negated2:\n",
    "                    x.append('IN')\n",
    "                if len(negated2) > 0:\n",
    "                    scoped += negated2\n",
    "    return scoped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imperative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imp(df, candidates):\n",
    "    verb = df.loc[df['pos']=='VERB']\n",
    "    if verb.empty is False:\n",
    "        for index, row in verb.iterrows():\n",
    "            try:\n",
    "                i = row['gram']['Mood']\n",
    "                if i == 'Imp':\n",
    "                    scoped = scope_down(df, int(row['t_id']))\n",
    "                    for x in scoped:\n",
    "                        x.append('IMP')\n",
    "                    return scoped\n",
    "                else:\n",
    "                    return []\n",
    "            except (KeyError, TypeError):\n",
    "                return []\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quest(df, token):\n",
    "    scoped = []\n",
    "    question = df.loc[df['t_id'] < token]\n",
    "    #print(question)\n",
    "    scope_border = question.loc[question['lemma']=='-']\n",
    "    if scope_border.empty is False:\n",
    "        if len(scope_border) > 1:\n",
    "            q_ids = list(scope_border['t_id'])\n",
    "            #print(q_ids[0])\n",
    "            question = question.loc[question['t_id'] > q_ids[0]]\n",
    "            #print(question)\n",
    "        else:\n",
    "            question = question.loc[question['t_id'] > int(scope_border['t_id'])]\n",
    "    for index, row in question.iterrows():\n",
    "        if row['pos'] == 'VERB' or row['pos'] == 'ADV' or row['pos'] == 'PRON':\n",
    "            scoped.append([row['lemma'], row['pos'], row['t_id'], 'QUEST'])\n",
    "    return scoped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Irrealis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irr(df, token):\n",
    "    scoped = scope_up1(df, token)\n",
    "    scoped.append('IRR')\n",
    "    return scoped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All together: check if there is a licenser and what is in its scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "markers_d = dict(не='DN', нет='DN', нету='DN', только='RESTR', некого='DNN', нечего='DNN', если='COND', якобы='IRR', бы='CONDIRR', мочь='MODAL', хотеть='MODAL', должен='MODAL', обязан='MODAL', вынудить='MODAL', вынужденный='MODAL', надо='MODAL', можно='MODAL', хотеться='MODAL', заставлять='MODAL', заставить='MODAL')\n",
    "markers_l = list(markers_d.keys())\n",
    "markers_l.append('?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an item or item's occurrence to the dictionary of candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_candidates(x, candidates, scoped):\n",
    "    if deja_scoped(x[2], scoped) is False:\n",
    "        if x[0] not in candidates:\n",
    "            candidates[x[0]] = Candidate(x[0], x[1])\n",
    "        candidates[x[0]].contexts[x[3]] += 1\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deja_scoped(token, scoped):\n",
    "    if token in scoped:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply all to a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>head</th>\n",
       "      <th>form</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>gram</th>\n",
       "      <th>is_in_scope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Но</td>\n",
       "      <td>но</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>всё</td>\n",
       "      <td>все</td>\n",
       "      <td>PRON</td>\n",
       "      <td>{'Animacy': 'Inan', 'Case': 'Nom', 'Gender': '...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>же</td>\n",
       "      <td>же</td>\n",
       "      <td>PART</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>интересно</td>\n",
       "      <td>интересный</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>листовки</td>\n",
       "      <td>листовка</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>то</td>\n",
       "      <td>то</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>такой</td>\n",
       "      <td>такой</td>\n",
       "      <td>DET</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>не</td>\n",
       "      <td>не</td>\n",
       "      <td>PART</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>было</td>\n",
       "      <td>быть</td>\n",
       "      <td>VERB</td>\n",
       "      <td>{'Aspect': 'Imp', 'Gender': 'Neut', 'Mood': 'I...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    t_id  head       form       lemma    pos  \\\n",
       "0      1     4         Но          но  CCONJ   \n",
       "1      2     4        всё         все   PRON   \n",
       "2      3     2         же          же   PART   \n",
       "3      4     0  интересно  интересный    ADJ   \n",
       "4      5     4          -           -  PUNCT   \n",
       "5      6     4   листовки    листовка   NOUN   \n",
       "6      7     6         то          то  SCONJ   \n",
       "7      8     6      такой       такой    DET   \n",
       "8      9     8         не          не   PART   \n",
       "9     10     8       было        быть   VERB   \n",
       "10    11    10          .           .  PUNCT   \n",
       "\n",
       "                                                 gram is_in_scope  \n",
       "0                                                None        None  \n",
       "1   {'Animacy': 'Inan', 'Case': 'Nom', 'Gender': '...        None  \n",
       "2                                                None        None  \n",
       "3                                                None        None  \n",
       "4                                                None        None  \n",
       "5                                                None        None  \n",
       "6                                                None        None  \n",
       "7                                                None        None  \n",
       "8                                                None        None  \n",
       "9   {'Aspect': 'Imp', 'Gender': 'Neut', 'Mood': 'I...        None  \n",
       "10                                               None        None  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = parsed_sentences[9000]\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_marked(df, markers_d, markers_l, candidates):\n",
    "    tokens_scoped = []\n",
    "    dfs = separate_clauses(df)\n",
    "    for df in dfs:\n",
    "        mm = df.loc[df['lemma'].isin(markers_l)]\n",
    "        if mm.empty is False:\n",
    "            for n in list(mm['t_id']):\n",
    "                m = df.loc[df['t_id']==n, 'lemma'].max()\n",
    "                if m == '?':\n",
    "                    m_h = df.loc[df['lemma']==m, 't_id']\n",
    "                    for j in list(m_h):\n",
    "                        scoped = get_quest(df, j)\n",
    "                        for x in scoped:\n",
    "                            candidates = add_to_candidates(x, candidates, tokens_scoped)\n",
    "                elif markers_d[m] is not None:\n",
    "                    if markers_d[m] == 'DN':\n",
    "                        scoped = dir_neg(df, n)\n",
    "                        for x in scoped:\n",
    "                            candidates = add_to_candidates(x, candidates, tokens_scoped)\n",
    "                    elif markers_d[m] == 'DNN':\n",
    "                        scoped = scope_down(df, n)\n",
    "                        for x in scoped:\n",
    "                            x.append('DNN')\n",
    "                            candidates = add_to_candidates(x, candidates, tokens_scoped)\n",
    "                    elif markers_d[m] == 'CONDIRR':\n",
    "                        scoped = scope_up1(df, n)\n",
    "                        scoped.append('CONDIRR')\n",
    "                        candidates = add_to_candidates(scoped, candidates, tokens_scoped)  \n",
    "                    elif markers_d[m] == 'COND':\n",
    "                        scoped = scope_up1(df, n)\n",
    "                        scoped.append('COND')\n",
    "                        candidates = add_to_candidates(scoped, candidates, tokens_scoped)\n",
    "                    elif markers_d[m] == 'IRR':\n",
    "                        scoped = get_irr(df, n)\n",
    "                        if len(scoped) > 0:\n",
    "                            candidates = add_to_candidates(scoped, candidates, tokens_scoped)\n",
    "                    elif markers_d[m] == 'MODAL':\n",
    "                        if m == 'мочь':\n",
    "                            if df.loc[df['t_id']==(+1), 'lemma'].max() in [',', 'быть']:\n",
    "                                break\n",
    "                            else:\n",
    "                                scoped = scope_down(df, n)\n",
    "                                for x in scoped:\n",
    "                                    x.append('MODAL')\n",
    "                                    candidates = add_to_candidates(x, candidates, tokens_scoped)\n",
    "                        else:\n",
    "                            scoped = scope_down(df, n)\n",
    "                            for x in scoped:\n",
    "                                x.append('MODAL')\n",
    "                                candidates = add_to_candidates(x, candidates, tokens_scoped)     \n",
    "                    elif markers_d[m] == 'RESTR':\n",
    "                        scoped = [scope_up1(df, n)]\n",
    "                        if scoped[0][1] != 'VERB':\n",
    "                            sc = df.loc[df['lemma']==scoped[0][0], 't_id']\n",
    "                            #print(list(sc))\n",
    "                            for c in sc:\n",
    "                                #print(row['t_id'])\n",
    "                                #scoped_id = int(df.loc[df['lemma']==scoped[0][0], 't_id'])\n",
    "                                scoped.append(scope_up1(df, c))\n",
    "                            #print(type(scoped[0][0]))\n",
    "                            try:\n",
    "                                if scoped[1][1] != 'VERB' and type(scoped[0][0]) is not float:\n",
    "                                #if type(scoped[1][1]) is not float:\n",
    "                                    scoped_id = list(df.loc[df['lemma']==scoped[1][0], 't_id'])\n",
    "                                    for z in scoped_id:\n",
    "                                        scoped.append(scope_up1(df, z))\n",
    "                            except IndexError:\n",
    "                                pass\n",
    "                        for x in scoped:\n",
    "                            if x[1] == 'VERB' or x[1]=='ADV' or x[1]=='PRON':\n",
    "                                x.append('RESTR')\n",
    "                                candidates = add_to_candidates(x, candidates, tokens_scoped)   \n",
    "        impered = get_imp(df, candidates)\n",
    "        if len(impered) > 0:\n",
    "            for i in impered:\n",
    "                candidates = add_to_candidates(i, candidates, tokens_scoped)\n",
    "                    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentences from .conllu files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720587"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sents(fname):\n",
    "    sentences = []\n",
    "    text= open(fname, 'r', encoding='utf-8').read()\n",
    "    sents = re.findall('sent_id = (\\S+)\\n# text = ([^\\n]+)\\n([^#]+)\\n\\n', text, flags=re.DOTALL)\n",
    "    for sent in sents:\n",
    "        sentence =  sent[2]\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "#texto = open('tst_conllu.conllu', 'r', encoding='utf-8').read()\n",
    "sentences = get_sents('rus/ru_syntagrus-ud-dev.conllu')\n",
    "sentences += get_sents('rus/vktexts.txt')\n",
    "sentences += get_sents('rus/ru_taiga-ud-train.conllu')\n",
    "sentences += get_sents('rus/ru_syntagrus-ud-train.conllu')\n",
    "sentences += get_sents('rus/ru_syntagrus-ud-train.conllu')\n",
    "sentences += get_sents('rus/ru-ud-train.conllu')\n",
    "sentences += get_sents('rus/ru-ud-test.conllu')\n",
    "sentences += get_sents('rus/ru-ud-dev.conllu')\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...and do the thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720587"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_sentences = []\n",
    "for s in sentences:\n",
    "    parsed_s = parsed_to_df(parse_sent(s))\n",
    "    parsed_sentences.append(parsed_s)\n",
    "len(parsed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      ".....\n",
      "601500\n",
      "602000\n",
      "602500\n",
      "603000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603500\n",
      ".....\n",
      "720500\n"
     ]
    }
   ],
   "source": [
    "candidates = {}\n",
    "for s in range(len(parsed_sentences)):\n",
    "    if s % 500 == 0:\n",
    "        print(s)\n",
    "    #print(s)\n",
    "    candidates = is_marked(parsed_sentences[s], markers_d, markers_l, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355125"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lemmas = count_all_lemmas(parsed_sentences)\n",
    "len(all_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_more_lemmas(parsed_sentences, parsed_sentences2):\n",
    "    all_lemmas = []\n",
    "    for x in parsed_sentences:\n",
    "        all_lemmas.append(list(x['lemma']))\n",
    "    for y in parsed_sentences2:\n",
    "        all_lemmas.append(list(y['lemma']))\n",
    "    all_lemmas = list(itertools.chain(*all_lemmas))\n",
    "    lemmas = collections.Counter(all_lemmas)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = ['!', '?', ',', '.', '\"', '-', '~', ')', '(', ':', ';']\n",
    "for p in punct:\n",
    "    all_lemmas.pop(p, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10082464"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(all_lemmas.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_lemmas = candidates.keys()\n",
    "for x in cand_lemmas:\n",
    "    candidates[x].all_occ = all_lemmas[x]\n",
    "    candidates[x].all_nonver = sum(list(candidates[x].contexts.values()))\n",
    "    candidates[x].all_other = candidates[x].all_occ - candidates[x].all_nonver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19794"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbal_candidates = []\n",
    "for v in cand_lemmas: \n",
    "    if candidates[v].pos == 'VERB':\n",
    "        verbal_candidates.append(candidates[v])\n",
    "len(verbal_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apresyan = ['житься', 'запомнить', 'обобраться', 'терпеться', 'подумать', 'посмотреть', 'задуматься', 'замедлить', 'обинуясь', 'преминуть', 'заладиться', 'миновать', 'пара', 'напастись', 'надивиться', 'наздравствоваться', 'сроду', 'выносить', 'плошать', 'скупиться.', 'клеиться', 'взвидеть', 'удосужиться', 'стерпеть', 'притронуться', 'сидеться', 'наготовиться', 'ведать', 'переваривать', 'накупиться', 'рыпаться', 'впервой', 'допроситься', 'дозваться', 'трогать', 'видать', 'видаться', 'поддаваться', 'сметь', 'досмотреть', 'лежаться', 'писаться', 'тронуть', 'прикоснуться', 'доглядеть', 'годиться', 'пристать', 'задаться', 'нюхать', 'сходить', 'выдержать', 'спросить', 'угнаться', 'браться', 'навоевать', 'повинный', 'поверить', 'стесняться', 'вытерпеть', 'уколупнуть', 'укупить', 'справиться', 'гадать', 'пропасть', 'трогать', 'полагаться', 'положить', 'переносить', 'постыдить', 'терпеть']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_strong = ['житься', 'запомнить', 'обобраться', 'терпеться', 'подумать', 'посмотреть', 'задуматься', 'замедлить', 'обинуясь', 'преминуть', 'заладиться', 'миновать', 'пара', 'напастись', 'надивиться', 'наздравствоваться', 'сроду', 'выносить', 'плошать', 'скупиться.', 'клеиться', 'взвидеть', 'удосужиться', 'стерпеть', 'притронуться', 'сидеться', 'наготовиться', 'ведать', 'переваривать', 'накупиться', 'рыпаться']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "выносить 463 - 131\n",
      "{'DN': 42, 'DNN': 0, 'RESTR': 9, 'COND': 19, 'CONDIRR': 8, 'QUEST': 4, 'MODAL': 49, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "подумать 1039 - 309\n",
      "{'DN': 73, 'DNN': 0, 'RESTR': 26, 'COND': 22, 'CONDIRR': 18, 'QUEST': 10, 'MODAL': 136, 'IN': 0, 'IMP': 24, 'IRR': 0}\n",
      "ведать 108 - 51\n",
      "{'DN': 45, 'DNN': 0, 'RESTR': 3, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 2, 'IN': 0, 'IMP': 0, 'IRR': 1}\n",
      "задуматься 97 - 21\n",
      "{'DN': 1, 'DNN': 0, 'RESTR': 4, 'COND': 3, 'CONDIRR': 1, 'QUEST': 5, 'MODAL': 7, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "стерпеть 4 - 1\n",
      "{'DN': 0, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 1, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "рыпаться 20 - 13\n",
      "{'DN': 11, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 2, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "преминуть 13 - 17\n",
      "{'DN': 13, 'DNN': 0, 'RESTR': 0, 'COND': 2, 'CONDIRR': 2, 'QUEST': 0, 'MODAL': 0, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "удосужиться 11 - 12\n",
      "{'DN': 12, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 0, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "переваривать 23 - 22\n",
      "{'DN': 19, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 3, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "житься 33 - 5\n",
      "{'DN': 1, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 1, 'QUEST': 0, 'MODAL': 3, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "пара 1328 - 13\n",
      "{'DN': 11, 'DNN': 0, 'RESTR': 0, 'COND': 1, 'CONDIRR': 1, 'QUEST': 0, 'MODAL': 0, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "миновать 71 - 18\n",
      "{'DN': 13, 'DNN': 0, 'RESTR': 0, 'COND': 2, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 3, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "клеиться 8 - 8\n",
      "{'DN': 7, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 1, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "сидеться 4 - 3\n",
      "{'DN': 3, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 0, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "заладиться 3 - 2\n",
      "{'DN': 2, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 0, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "посмотреть 9 - 3\n",
      "{'DN': 1, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 1, 'IN': 0, 'IMP': 1, 'IRR': 0}\n",
      "запомнить 46 - 8\n",
      "{'DN': 3, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 5, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "обобраться 4 - 4\n",
      "{'DN': 4, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 0, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "надивиться 2 - 2\n",
      "{'DN': 0, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 2, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "замедлить 10 - 4\n",
      "{'DN': 0, 'DNN': 0, 'RESTR': 0, 'COND': 2, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 2, 'IN': 0, 'IMP': 0, 'IRR': 0}\n",
      "терпеться 8 - 8\n",
      "{'DN': 8, 'DNN': 0, 'RESTR': 0, 'COND': 0, 'CONDIRR': 0, 'QUEST': 0, 'MODAL': 0, 'IN': 0, 'IMP': 0, 'IRR': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0\n",
    "shared = []\n",
    "for x in cand_lemmas:\n",
    "    v = candidates[x]\n",
    "    if v.word in apr_strong:\n",
    "        shared.append(v.word)\n",
    "        print(v.word + ' ' + str(v.all_occ) + ' - ' + str(v.all_nonver))\n",
    "        print(v.contexts)\n",
    "        k+=1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for y in cand_lemmas:\n",
    "    ca = candidates[y]\n",
    "    info = {'lemma': ca.word, 'pos': ca.pos, 'DNN': ca.contexts['DNN'],'DN': ca.contexts['DN'],\n",
    "           'RESTR': ca.contexts['RESTR'],'COND': ca.contexts['COND'],'CONDIRR': ca.contexts['CONDIRR'],\n",
    "            'QUEST': ca.contexts['QUEST'],'MODAL': ca.contexts['MODAL'],'IN': ca.contexts['IN'],\n",
    "            'IMP': ca.contexts['IMP'],'IRR': ca.contexts['IRR'], 'nonver': ca.all_nonver, 'all_occ': ca.all_occ, 'all_other': ca.all_other}\n",
    "    results.append(info)\n",
    "#len(results)\n",
    "res_df = pd.DataFrame.from_dict(results)\n",
    "res_file = res_df.to_csv('npi_results2.csv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
